{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alignment analysis in ROIs  \n",
    "output: df_rois_similarity.csv  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, makedirs, walk, remove, getlogin, rename\n",
    "from os.path import join, exists, isfile, getmtime, isdir\n",
    "import numpy as np\n",
    "import re\n",
    "from brainiak import image, io, isc\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import brainiak.searchlight.searchlight as searchlight\n",
    "from brainiak.funcalign.srm import SRM\n",
    "import nibabel as nib\n",
    "import time\n",
    "import sys\n",
    "from scipy import stats\n",
    "import socket\n",
    "import pandas as pd\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "try:  # in jupyter - load tools and run test\n",
    "    from IPython.display import display\n",
    "    import matplotlib.pyplot as plt\n",
    "    get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina' # for 4k screen\")\n",
    "    from IPython.core.interactiveshell import InteractiveShell  # for var view\n",
    "    InteractiveShell.ast_node_interactivity = \"all\"  # for var view\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib import rcParams\n",
    "    rcParams.update({'figure.autolayout': True})\n",
    "    import pdb\n",
    "    import seaborn as sns\n",
    "    use_test_params=True # also clips number of voxels, skips searchlight\n",
    "except ImportError:  # not jupyter\n",
    "    print('Error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server: scotty.pni.Princeton.EDU\n",
      "DATA: /mnt/bucket/labs/hasson/meshulam/onlineL/pred20\n",
      "CODE: /mnt/bucket/people/meshulam/meshul/notebooks/share\n",
      "PICKLES:/mnt/bucket/labs/hasson/meshulam/onlineL/shared/pickles\n"
     ]
    }
   ],
   "source": [
    "#set paths\n",
    "my_name = getlogin()\n",
    "\n",
    "# set system\n",
    "this_system = socket.gethostname()\n",
    "print ('Server: '+this_system)\n",
    "\n",
    "# DATA\n",
    "bids_path='/mnt/sink/scratch/{}/to_bids'.format(my_name) # raw data\n",
    "const_data_path = '/mnt/bucket/labs/hasson/'+my_name\n",
    "const_study_path=join(const_data_path,'onlineL','pred20') # pre-processed data\n",
    "input_fslfeat_students_path=join(const_study_path,'scan_data_nii','students_mni','6motion') # student pre-processed data\n",
    "input_fslfeat_experts_path=join(const_study_path,'scan_data_nii','experts_mni','6motion') # expert pre-processed data \n",
    "\n",
    "\n",
    "print('DATA: ' + const_study_path)\n",
    "\n",
    "# CODE\n",
    "code_path='/mnt/bucket/people/{}/{}/notebooks/share'.format(my_name,my_name[:-2])\n",
    "print('CODE: ' + code_path)\n",
    "# SCORES\n",
    "scores_path=join(bids_path,'sourcedata','exam_scores.tsv')\n",
    "timing_path=join(bids_path,'sourcedata','exam_timing.tsv')\n",
    "# MASKS\n",
    "masks_path=join(code_path,'masks')\n",
    "const_mni_brain_file_name = join(masks_path, 'MNI152_T1_3mm_brain.nii.gz')  # MNI brain\n",
    "\n",
    "# PICKLES\n",
    "pickles_path=join(const_data_path,'onlineL','shared','pickles')\n",
    "print('PICKLES:' + pickles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiple comparisons tools\n",
    "sys.path.insert(0, join(code_path, 'py'))\n",
    "from multi_comp import fdr_correction # FDR from the MNE-python package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rois\n",
    "use_roi=True\n",
    "load_roi_dict=True\n",
    "if load_roi_dict:\n",
    "    pickle_filename=join(pickles_path,'thr20_roi_dict.p') # saved during lecture/recaps processing\n",
    "    masks=pickle.load(open(pickle_filename, 'rb')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper func definitions (read/save data, run correlations, permutation test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_filenames(student_and_expert_files):\n",
    "    \"\"\"\n",
    "    build dict for all filenames, nested\n",
    "    :param student_and_expert_files: from listdir\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    filenames_dict = dict()\n",
    "    filenames_dict['students'] = {}\n",
    "    filenames_dict['experts'] = {}\n",
    "    for i_this_file, this_file in enumerate(student_and_expert_files):\n",
    "        dk = re.search(task_name_template, this_file)\n",
    "        this_subject = str(dk[1])\n",
    "        this_session = str(dk[2])\n",
    "        this_task = str(dk[3])\n",
    "        if i_this_file >= len(listdir(input_fslfeat_students_path)):\n",
    "            student_or_expert = 'experts'\n",
    "        else:\n",
    "            student_or_expert = 'students'\n",
    "        try:\n",
    "            temp = type(filenames_dict[student_or_expert][this_subject]) is dict\n",
    "        except KeyError:\n",
    "            filenames_dict[student_or_expert][this_subject] = {}\n",
    "        try:\n",
    "            temp = type(filenames_dict[student_or_expert][this_subject][this_session]) is dict\n",
    "        except KeyError:\n",
    "            filenames_dict[student_or_expert][this_subject][this_session] = {}\n",
    "        # write filename to dict\n",
    "        filenames_dict[student_or_expert][this_subject][this_session][this_task] = join(\n",
    "            eval('input_fslfeat_{}_path'.format(student_or_expert)), this_file)\n",
    "    # filenames dict structure:\n",
    "    # filenames_dict['students']['s110']['wk2']['vid1']\n",
    "    return filenames_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filenames_to_SRM_input_list(input_filenames, brain_mask):    \n",
    "    # run once for each of (train, test)\n",
    "    # take filenames and build a data structure (list) for input to SRM, zscored per subject, no nans\n",
    "    # input:\n",
    "    # filenames (list, text)\n",
    "    # brain_mask (assumes it was already read with io.load_boolean_mask and thresholded)\n",
    "    # outputs:\n",
    "    # output_list - SRM input\n",
    "    # function run time\n",
    "\n",
    "    # We will divide the work (and memory) of loading subject data accross ranks.\n",
    "    subject_idx_list = np.array_split([iSubject for iSubject in range(len(input_filenames))], comm.size)[comm.rank]\n",
    "    print(\"Rank {}: Loading subjects -> {}\".format(comm.rank, subject_idx_list))\n",
    "    # on each rank, we need a list of the same size with None in place of subject data that is loaded on other ranks\n",
    "    output_list = [None for i in input_filenames]\n",
    "    t1 = time.time()  # timeit\n",
    "\n",
    "    for iSubject in subject_idx_list:\n",
    "        # load\n",
    "        this_image = nib.load(input_filenames[iSubject])\n",
    "        # mask\n",
    "        this_image_masked = image.mask_image(this_image, brain_mask)\n",
    "        # nan to 0\n",
    "        this_image_masked[np.isnan(this_image_masked)] = 0\n",
    "        # zscore\n",
    "        this_image_masked_zscored = stats.zscore(this_image_masked, axis=1, ddof=1)\n",
    "        # into x*y*z*tr format\n",
    "        uncon = np.zeros(\n",
    "            (brain_mask.shape[0], brain_mask.shape[1], brain_mask.shape[2], this_image_masked_zscored.shape[1]))\n",
    "        coords = np.where(brain_mask)\n",
    "        uncon[coords[0], coords[1], coords[2], :] = np.nan_to_num(this_image_masked_zscored)\n",
    "        # update output\n",
    "        output_list[iSubject] = uncon\n",
    "    t2 = time.time()\n",
    "    return output_list, t2 - t1  # data and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_isc_in_notebook(D, collapse_subj=True, external_signal=None, float_type=np.float16):\n",
    "    \"\"\"\n",
    "    Internal ISC function for this notebook, no stats\n",
    "    external_signal: signal to correlate with, instead of mean subjects (n-1) like in standard isc\n",
    "    # but here added external mean for comparison (mean of experts to compare to the individual students)\n",
    "    \"\"\"\n",
    "    n_vox = D.shape[0]\n",
    "    n_subj = D.shape[2]\n",
    "    ISC = np.zeros((n_vox, n_subj), dtype=float_type)\n",
    "    for this_subject in range(n_subj):\n",
    "        if external_signal is not None:\n",
    "            group = external_signal\n",
    "            assert np.logical_and(group.shape[0] == D.shape[0],\n",
    "                                  group.shape[1] == D.shape[1]), 'dims mismatch for external signal input'\n",
    "        else:\n",
    "            group = np.mean(D[:, :, np.arange(n_subj) != this_subject], axis=2)\n",
    "        subj = D[:, :, this_subject]\n",
    "        for v in range(n_vox):\n",
    "            ISC[v, this_subject] = stats.pearsonr(group[v, :], subj[v, :])[0]\n",
    "    \n",
    "    if collapse_subj:\n",
    "        ISC = isc.compute_summary_statistic(ISC, axis=1)[np.newaxis, :]\n",
    "    # Throw away first dimension if singleton\n",
    "    if ISC.shape[0] == 1:\n",
    "        ISC = ISC[0]\n",
    "    return ISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_isfc_in_notebook(D, collapse_subj=True, external_signal=None, float_type=np.float16):\n",
    "    \"\"\"\n",
    "    helper func, internal ISFC for this notebook, no stats\n",
    "    external_signal: signal to correlate with, instead of mean subjects (n-1) like in standard isfc\n",
    "    # but here added external mean for comparison (mean of experts to compare to the individual students)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_vox = D.shape[0]\n",
    "    n_subj = D.shape[2]\n",
    "    ISFC = np.zeros((n_vox, n_vox, n_subj),dtype=float_type)\n",
    "\n",
    "    # Loop across choice of leave-one-out subject\n",
    "    for this_subject in range(n_subj):\n",
    "        if external_signal is not None:\n",
    "            group = external_signal\n",
    "            assert np.logical_and(group.shape[0] == D.shape[0],\n",
    "                                  group.shape[1] == D.shape[1]), 'dims mismatch for external signal input'\n",
    "        else:\n",
    "            group = np.mean(D[:, :, np.arange(n_subj) != this_subject], axis=2)\n",
    "        subj_data = D[:, :, this_subject]\n",
    "        ISFC[:, :, this_subject] = compute_correlation(np.ascontiguousarray(subj_data),np.ascontiguousarray(group)) # order important because compute_correlation correlates rows of matrix 1 with rows of matrix 2\n",
    "\n",
    "        # Symmetrize matrix - skip\n",
    "        #if external_signal is None:\n",
    "        #    ISFC[:, :, this_subject] = (ISFC[:, :, this_subject] +\n",
    "        #                            ISFC[:, :, this_subject].T) / 2\n",
    "    # collapse over subjects\n",
    "    if collapse_subj:\n",
    "        ISFC = isc.compute_summary_statistic(ISFC, axis=2)\n",
    "    # Throw away first dimension if singleton\n",
    "    if ISFC.shape[0] == 1:\n",
    "        ISFC = ISFC[0]\n",
    "\n",
    "    return ISFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_corr_and_null_dist(x,y,num_perms=0):\n",
    "    \"\"\"\n",
    "    returns rval, null distribution for correlation of x,y\n",
    "    \"\"\"\n",
    "    null_dist=np.nan\n",
    "    rval = stats.pearsonr(x,y)[0]\n",
    "    pval = 1\n",
    "    if num_perms>0:\n",
    "        null_dist=np.array([stats.pearsonr(np.random.permutation(x), y)[0] for n in np.arange(num_perms)])\n",
    "    return rval,null_dist\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files_from_dict(filenames_dict, students_or_experts, week, vid_name):\n",
    "    subject_keys = sorted([k for k, v in filenames_dict[students_or_experts].items()])\n",
    "    return_files = [None for i in range(len(subject_keys))]\n",
    "    for i_this_student in range(len(subject_keys)):\n",
    "        try:\n",
    "            return_files[i_this_student] = (\n",
    "            filenames_dict[students_or_experts][subject_keys[i_this_student]][week][vid_name])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return [i for i in return_files if i]  # eliminate nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_filenames(list1, list2):\n",
    "    # omit filenames from list1,list2, for subjects that do not appear in both lists\n",
    "    # params:\n",
    "    # list1 - file names\n",
    "    # list2 - file names or list of subject names\n",
    "    # output:\n",
    "    # files_out_1 - list1 items, minus subjects that are not in list 2\n",
    "    # files_out_2 - list2 items, minus subjects that are not in list 1 [only returned in case list2 contained files, otherwise not retured]\n",
    "    list2_is_file_names = False\n",
    "    subjects_in_1 = [str(re.search(task_name_template, f)[1]) for f in list1]\n",
    "    try:\n",
    "        subjects_in_2 = [str(re.search(task_name_template, f)[1]) for f in list2]\n",
    "        list2_is_file_names = True\n",
    "    except TypeError:  # not a list of files matching template, use subject names in list2 directly\n",
    "        subjects_in_2 = list2\n",
    "    shared_subjects = list(set(subjects_in_1) & set(subjects_in_2))  # find subjects in both sets\n",
    "    files_out_1 = [f for f in list1 if str(re.search(task_name_template, f)[1]) in shared_subjects]\n",
    "    if not list2_is_file_names:\n",
    "        return files_out_1\n",
    "    else:\n",
    "        files_out_2 = [f for f in list2 if str(re.search(task_name_template, f)[1]) in shared_subjects]\n",
    "    return files_out_1, files_out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_placement_logs(timing_path=timing_path,scores_path=scores_path):\n",
    "    # read exam scores\n",
    "    # returns:\n",
    "    # df_q_timestamps - time stamp per question (most of the code is to get that)\n",
    "    # placement_by_q - grades\n",
    "    # get exam data\n",
    "    scores_df=pd.read_csv(scores_path,sep='\\t',index_col=[0])\n",
    "    scores_df.columns=range(16)\n",
    "    scores_df.index.name='Subject'\n",
    "    students_ind=[True if int(i[1:])<200 else False for i,s in scores_df.iterrows() ]\n",
    "    experts_ind=np.logical_not(students_ind)\n",
    "    placement_by_q={}\n",
    "    placement_by_q['students']=scores_df[students_ind]\n",
    "    placement_by_q['experts']=scores_df[experts_ind]\n",
    "    placement_by_q['students']=placement_by_q['students'].drop(['s112'], axis=0) # no placement data\n",
    "    placement_by_q['experts']=placement_by_q['experts'].drop(['s201'], axis=0) # no placement data\n",
    "    \n",
    "    timing_df=pd.read_csv(timing_path,sep='\\t',index_col=[0])\n",
    "    timing_df['subject']=timing_df.index\n",
    "    timing_df['student_or_expert']=['student' if int(i[1:])<200 else 'expert' for i,s in timing_df.iterrows() ]\n",
    "    timing_df= timing_df.rename(columns={'question': 'q_number','response_onset_TR':'q_start_TR','response_offset_TR':'q_end_TR'})\n",
    "    timing_df['q_number']-=1\n",
    "    timing_df.subject=timing_df.index\n",
    "    \n",
    "    return timing_df,placement_by_q\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchlight info printout\n",
    "def print_sl_info(epi_list,sl_rad,nfeature,loc_split):\n",
    "    print ('Searchlight cube edge size is {} -> {} voxels'.format(1 + 2 * sl_rad, (1 + 2 * sl_rad) ** 3))\n",
    "    if do_srm:\n",
    "        print ('Number of features in SRM: {}'.format(nfeature))\n",
    "        print ('Number of TRs in training data: {}'.format(epi_list[0].shape[3]))\n",
    "        print ('Number of TRs in test data: {}'.format(epi_list[int(len(epi_list) / 2)].shape[3]))\n",
    "        print ('Number of subjects: {}'.format(int(len(epi_list) / 2)))\n",
    "        print ('Number of student datasets: {}, expert datasets: {}.'.format(loc_split, len(epi_list) / 2 - loc_split))\n",
    "    else:\n",
    "        print ('Number of subjects: {}'.format(int(len(epi_list))))\n",
    "        print ('Number of student datasets: {}, expert datasets: {}.'.format(loc_split, len(epi_list) - loc_split))\n",
    "        print ('Number of TRs in first list item: {}'.format(epi_list[0].shape[3]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(suffix=None):\n",
    "    \"\"\"\n",
    "    :params: suffix - last part of file name\n",
    "    :return: name of file to save, no extension\n",
    "    \"\"\"\n",
    "    file_name=''\n",
    "    if 'wk' in sys.argv[1]:\n",
    "        file_name+='SRM-train_'+train_vid_week+'-'+train_vid_name+'-test-'\n",
    "    else:\n",
    "        file_name+='data-'\n",
    "    file_name+=test_vid_week+'-'+test_vid_name+'_search-'\n",
    "    file_name+=str((1 + 2 * sl_rad) ** 3)+'vox_'\n",
    "    file_name+= similarity_type+'_'+vs_mean_of+'_corrwscore-'+correlation_with_score+'_'\n",
    "    file_name+='perms={}'.format(num_perms)\n",
    "    if suffix:\n",
    "        file_name+='_'+suffix\n",
    "    return file_name\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epi_data(subject_groups):\n",
    "    \"\"\"\n",
    "    load dict_epi_data dict\n",
    "    :return: dictionary with all epi data for the subject groups specified\n",
    "    \"\"\"\n",
    "    dict_epi_data = {}\n",
    "    dict_epi_data['students']={}\n",
    "    dict_epi_data['experts']={}\n",
    "    dict_epi_data['students']['test_data']=[]\n",
    "    dict_epi_data['experts']['test_data']=[]\n",
    "   \n",
    "    for students_or_experts in sorted(subject_groups):\n",
    "        if rank == 0:\n",
    "            print(students_or_experts)\n",
    "\n",
    "        # get filenames for test, (+train if required)\n",
    "        test_on_files_prep = get_list_of_files_from_dict(filenames_dict, students_or_experts, test_vid_week,\n",
    "                                                         test_vid_name)\n",
    "        if train_vid_week:\n",
    "            train_on_files_prep = get_list_of_files_from_dict(filenames_dict, students_or_experts, train_vid_week,\n",
    "                             train_vid_name)\n",
    "        # get same subjects for train,test\n",
    "        # then get files, read into dict of lists\n",
    "        if 'student' in students_or_experts:  # get only good students\n",
    "            test_on_files_prep = intersect_filenames(test_on_files_prep, good_students)\n",
    "        elif 'expert' in students_or_experts:\n",
    "            test_on_files_prep = intersect_filenames(test_on_files_prep, good_experts)\n",
    "        # load train + intersect train-test + intersect with good students\n",
    "        if train_vid_week:\n",
    "            if 'student' in students_or_experts:\n",
    "                train_on_files_prep = intersect_filenames(train_on_files_prep, good_students)  # get only good students/experts\n",
    "            elif 'expert' in students_or_experts:\n",
    "                train_on_files_prep = intersect_filenames(train_on_files_prep, good_experts)  # get only good students/experts\n",
    "            train_on_files, test_on_files = intersect_filenames(train_on_files_prep,\n",
    "                                                                test_on_files_prep)  # intersect test-train\n",
    "            # load train\n",
    "            train_on_input, train_load_time = filenames_to_SRM_input_list(train_on_files, brain_mask_3mm)\n",
    "            dict_epi_data[students_or_experts]['train_data'] = train_on_input.copy()  # list of subjects, vox x trs\n",
    "        else:  # no train data\n",
    "            test_on_files = test_on_files_prep  # skip intersect with train\n",
    "        # load test\n",
    "        test_on_input, test_load_time = filenames_to_SRM_input_list(test_on_files, brain_mask_3mm)\n",
    "        dict_epi_data[students_or_experts]['test_data'] = test_on_input.copy()\n",
    "    if rank == 0:\n",
    "        print()\n",
    "        if train_vid_week:\n",
    "            print(\"Training set, {} subjects, load time: {:.2f}s\".format(len(train_on_input), train_load_time))\n",
    "        print(\"Test set, {} subjects, load time: {:.2f}s\".format(len(test_on_input), test_load_time))\n",
    "        # addition for ROI\n",
    "        print('Test on files:')\n",
    "        print(test_on_files)\n",
    "    return dict_epi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_roi_data(data_in_sl,roi_mask,roi_name,roi_hemi):\n",
    "    \"\"\"\n",
    "    Get ROI from whole-brain data in data_in_sl\n",
    "    \"\"\"\n",
    "    roi_mask=roi_mask>0 # binarize\n",
    "    print('ROI {}-{}: {} voxels'.format(roi_name,roi_hemi,np.sum(roi_mask)))\n",
    "    for st_ex in data_in_sl.keys():\n",
    "        for tr_ts in data_in_sl[st_ex].keys():\n",
    "            try:\n",
    "                print('{}-{}'.format(st_ex,tr_ts))\n",
    "                d1,d2,d3=roi_mask.shape\n",
    "                roi_mask_vec=np.reshape(roi_mask,(d1*d2*d3)) # vectorize mask\n",
    "                e=[i[roi_mask_vec,:] for i in data_in_sl[st_ex][tr_ts]]\n",
    "                data_in_sl[st_ex][tr_ts]=e.copy()\n",
    "                print('out shape: {}'.format(data_in_sl[st_ex][tr_ts][0].shape))\n",
    "            except IndexError:\n",
    "                print('Could not slice TRs for test in {}-{}'.format(st_ex,tr_ts))\n",
    "    return data_in_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core analyses  \n",
    "'diagonal_similarity' - same question analysis (mean across students)  \n",
    "'isfc_similarity' - knowledge structure analysis  (mean across students)\n",
    "  \n",
    "Per-item analyses   - full output  + regression coefficients (for plots)   \n",
    "'diagonal_similarity_peritem'   \n",
    "'isfc_similarity_peritem' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_diagonal_similarity(df_q_timestamps, placement_by_q, data_in_sl, sim_or_sim_minus_nots='sim'):\n",
    "    \"\"\"\n",
    "    same-question analysis\n",
    "    \"\"\"\n",
    "    number_of_questions = len(df_q_timestamps['q_number'].unique())  # 16 q in placement exam\n",
    "    corr_result = {}\n",
    "    for student_or_expert in sorted(subject_groups): # order is important- do expert before to allow comparison with student\n",
    "        epi_data = data_in_sl[student_or_expert + 's'][\n",
    "            'test_data']  # in no srm version, epi data is just the non-transformed test data\n",
    "        number_of_subjects = len(epi_data)\n",
    "        corr_result[student_or_expert] = {}\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = None  # timecourse data in roi, do isc on this\n",
    "        corr_result[student_or_expert][\n",
    "            'roi_isc_per_q_vs_group_rval'] = None  # r-value result of isc, questions are in the 'voxels' dim: student vs student, expert vs expert\n",
    "        corr_result[student_or_expert][\n",
    "            'roi_isc_per_q_vs_group_dist'] = None  # null dist, questions are in the 'voxels' dim: student vs student, expert vs expert\n",
    "        if 'student' in student_or_expert:\n",
    "            corr_result['student']['roi_isc_per_q_student_vs_expert_rval'] = None  # rval result of pseudo-isc, student vs expert\n",
    "            corr_result['student']['roi_isc_per_q_student_vs_expert_dist'] = None  # null dist of pseudo-isc, student vs expert\n",
    "            \n",
    "        epi_temporal_mean_per_q = np.zeros(\n",
    "            [number_of_questions, epi_data[0].shape[0], len(epi_data)])  # questions (instead of TR) X voxels X subjects\n",
    "        for this_question in range(number_of_questions):\n",
    "            # print(this_question)\n",
    "            # slice this question: entries for this question, all subjects (expert or student separately)\n",
    "            df_this_q = df_q_timestamps.loc[np.logical_and(df_q_timestamps['student_or_expert'] == student_or_expert,\n",
    "                                                           df_q_timestamps['q_number'] == this_question)]\n",
    "            df_this_q = df_this_q.sort_values(by=['subject'], ascending=True)\n",
    "            # start and end points for this question (TR), for each subject\n",
    "            trs_start = df_this_q['q_start_TR'].values + trs_to_add_to_start\n",
    "            trs_end = df_this_q['q_end_TR'].values + trs_to_add_to_end\n",
    "            assert np.sum((trs_end - trs_start) > 2), 'slice problem in {}-{}-{}'.format(student_or_expert,\n",
    "                                                                                            trs_start,trs_end)\n",
    "            # extract mean across TRs of epi data per question: vox X subjects\n",
    "            epi_temporal_mean_per_q[this_question, :, :] = np.array(\n",
    "                [np.nanmean(epi_data[i_subject][:, np.int(trs_start[i_subject]):np.int(trs_end[i_subject])], axis=1) for\n",
    "                 i_subject in np.arange(len(trs_start))]).transpose()\n",
    "            # assert not np.sum(np.isnan(epi_temporal_mean_per_q[this_question,:,:])), 'nans here in {}, q{}'.format(student_or_expert,this_question)\n",
    "        # update dict with epi data, mean over TRs\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = epi_temporal_mean_per_q\n",
    "        # run spatial isc with questions as first dim (get per-question value) (output: question X subject)\n",
    "        # student vs mean of student, expert-expert\n",
    "        # run isc\n",
    "        corr_result[student_or_expert]['roi_isc_per_q_vs_group_rval'] = func_isc_in_notebook(epi_temporal_mean_per_q,\n",
    "                                                                                        collapse_subj=False)\n",
    "        # get bootstrap dist\n",
    "        if num_perms>0:\n",
    "            d=corr_result[student_or_expert]['roi_isc_per_q_vs_group_rval'] # questions X subjects \n",
    "            corr_result[student_or_expert]['roi_isc_per_q_vs_group_dist']=np.zeros([number_of_subjects,num_perms])\n",
    "            for this_subject in np.arange(number_of_subjects):\n",
    "                observed, ci, pval, dist = isc.bootstrap_isc(d[:,this_subject], pairwise=False, summary_statistic='mean',n_bootstraps=num_perms, ci_percentile=0, random_state=None)\n",
    "                corr_result[student_or_expert]['roi_isc_per_q_vs_group_dist'][this_subject,:] = dist.squeeze()\n",
    "        else:\n",
    "            corr_result[student_or_expert]['roi_isc_per_q_vs_group_dist']=None\n",
    "\n",
    "        # student vs CLEAN mean of expert (omit patterns of expert that answered wrong)\n",
    "        if 'student' in student_or_expert:\n",
    "            if epi_data is not None:\n",
    "                try:\n",
    "                    expert_all = corr_result['expert']['roi_epi_per_q']  # get data\n",
    "                    if not (('questions' in test_vid_name) or ('placement' in test_vid_name)):\n",
    "                        # 1) non-clean version - for use when no questions are involved - running just on video - qualifies all 'answers'\n",
    "                        expert_clean_collapsed = np.nanmean(expert_all,axis=2)                  \n",
    "                    else:\n",
    "                        # 2) CLEAN version   \n",
    "                        clean_mask = (placement_by_q[\n",
    "                                          'experts'] >= expert_accept_question_threshold).values.transpose()  # mask out wrong answers\n",
    "                        # mask out bad expert responses, replace with nans\n",
    "                        for v in np.arange(expert_all.shape[1]):\n",
    "                            expert_all[:, v, :][np.invert(clean_mask)] = np.nan\n",
    "                        # collapse over experts\n",
    "                        expert_clean_collapsed = np.nanmean(\n",
    "                            np.array([expert_all[:, v, :] for v in np.arange(expert_all.shape[1])]),\n",
    "                            axis=2).transpose()\n",
    "                    # get rval\n",
    "                    corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_rval'] = func_isc_in_notebook(\n",
    "                        epi_temporal_mean_per_q, collapse_subj=False, external_signal=expert_clean_collapsed)\n",
    "\n",
    "\n",
    "                    # get bootstrap dist\n",
    "                    if num_perms>0:\n",
    "                        d=corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_rval'] # questions X students\n",
    "                        corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_dist']=np.zeros([number_of_subjects,num_perms])\n",
    "                        for this_subject in np.arange(number_of_subjects): # iter students\n",
    "                            observed, ci, pval, dist = isc.bootstrap_isc(d[:,this_subject], pairwise=False, summary_statistic='mean',n_bootstraps=num_perms, ci_percentile=0, random_state=None)\n",
    "                            corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_dist'][this_subject,:] = dist.squeeze()\n",
    "                    else:\n",
    "                        corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_dist']=None\n",
    "                    if 'nots' in sim_or_sim_minus_nots:  # skip nots if not required\n",
    "                        if 'experts' in vs_mean_of:\n",
    "                            # get rval\n",
    "                            corr_result[student_or_expert]['roi_isc_NOTS_per_q_student_vs_expert_rval'] = func_spatial_isc_q_minus_nots(\n",
    "                                epi_temporal_mean_per_q, external_signal=expert_clean_collapsed)\n",
    "                        elif 'students' in vs_mean_of:\n",
    "                            # get rval\n",
    "                            corr_result[student_or_expert]['roi_isc_NOTS_per_q_student_vs_student_rval'] = func_spatial_isc_q_minus_nots(\n",
    "                                epi_temporal_mean_per_q)\n",
    "\n",
    "                except KeyError: # no expert data\n",
    "                    # zero out student-expert\n",
    "                    corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_rval']=0\n",
    "                    corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_dist']=0\n",
    "                    \n",
    "    if 'experts' in vs_mean_of:\n",
    "        if 'nots' in sim_or_sim_minus_nots:\n",
    "            # similarity minus nots: if this is correlated with score, then the similarity-score link is question specific, not global\n",
    "            # no p-value here, because how to compute\n",
    "            cx = corr_result['student']['roi_isc_per_q_student_vs_expert_rval'] - corr_result['student']['roi_isc_NOTS_per_q_student_vs_expert_rval']  # similarity minus nots\n",
    "            px = None\n",
    "        else:  # similarity\n",
    "            #rval \n",
    "            cx = corr_result['student']['roi_isc_per_q_student_vs_expert_rval'].copy() # questions X students\n",
    "            # pval for the mean value, variance source: subjects\n",
    "            null_dist=np.tanh(np.nanmean(np.arctanh(corr_result['student']['roi_isc_per_q_student_vs_expert_dist']),axis=0)) # mean over subj\n",
    "            if np.isnan(np.nanmean(cx)) or np.sum(np.isnan(null_dist)):\n",
    "                px=np.nan\n",
    "            else:\n",
    "                px = isc.p_from_null(np.tanh(np.nanmean(np.arctanh(cx))),null_dist,side='right',exact=False)\n",
    "    elif 'students' in vs_mean_of:\n",
    "        cx = corr_result['student']['roi_isc_per_q_vs_group_rval']  # questions X students\n",
    "        # pval for the mean value, variance source: subjects\n",
    "        null_dist=np.tanh(np.nanmean(np.arctanh(corr_result['student']['roi_isc_per_q_vs_group_dist']),axis=0)) # mean over subj\n",
    "        if np.isnan(np.nanmean(cx)) or np.sum(np.isnan(null_dist)):\n",
    "            px=np.nan\n",
    "        else:\n",
    "            px = isc.p_from_null(np.tanh(np.nanmean(np.arctanh(cx))),null_dist,side='right',exact=False)\n",
    "\n",
    "    if 'within' in correlation_with_score.lower(): # correlate similarity score with placement score within subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[1]) # rvals\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[1])) # rand dist for each rval \n",
    "        for i_this_student in range(cx.shape[1]):\n",
    "            # rvals and distribution for each, taken from that subject\n",
    "            x = cx[:, i_this_student]\n",
    "            y = placement_by_q['students'].iloc[i_this_student].values # scores\n",
    "            # alternative y: response length instead of question score\n",
    "            #$y_response_length=(df_q_timestamps[df_q_timestamps.subject==good_students[this_student]]).sort_values(by='q_number')['q_RT'].values\n",
    "            corr_score_vec [i_this_student],corr_perm_dist[:, i_this_student] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "        # rval\n",
    "        rval = isc.compute_summary_statistic(corr_score_vec)\n",
    "        # pval\n",
    "        within_dist = np.tanh(np.nanmean(np.arctanh(corr_perm_dist),axis=1)) # mean over subjects for each rand perm\n",
    "        if np.isnan(rval) or np.sum(np.isnan(within_dist)):\n",
    "            pval=np.nan\n",
    "        else:\n",
    "            pval=isc.p_from_null(rval,within_dist,side='right',exact=False)\n",
    "        out_val=np.array([rval,pval])\n",
    "            \n",
    "    elif 'direct' in correlation_with_score.lower(): # correlate similarity to group and similarity to experts\n",
    "        x=corr_result['student']['roi_isc_per_q_student_vs_expert_rval'] # questions X students\n",
    "        y=corr_result['student']['roi_isc_per_q_vs_group_rval']   # questions X students\n",
    "        # corr separately in each bin\n",
    "        n_bins=x.shape[0]\n",
    "        direct_dists=np.zeros((n_bins,num_perms))\n",
    "        direct_rvals=np.zeros((n_bins))\n",
    "        for this_bin in range(n_bins):\n",
    "            direct_rvals[this_bin],direct_dists[this_bin,:]=\\\n",
    "                func_corr_and_null_dist(x[this_bin,:],y[this_bin,:],num_perms=num_perms) \n",
    "        rval=np.tanh(np.nanmean(np.arctanh(direct_rvals))) # take mean across bins\n",
    "        direct_mean_dist=np.tanh(np.nanmean(np.arctanh(direct_dists),axis=0))\n",
    "        if np.isnan(rval) or np.sum(np.isnan(direct_mean_dist)):\n",
    "            pval=np.nan\n",
    "        else:\n",
    "            pval=isc.p_from_null(rval,direct_mean_dist,side='right',exact=False)\n",
    "        #if save_direct_placement_perms:\n",
    "        #    out_val = [rval,direct_mean_dist] # for recaps direct comprison, save perms of placement, sl output is npz\n",
    "        #else:\n",
    "        out_val = np.array([rval,pval]) # for placement direct comparison  - sl output is nii map\n",
    "    \n",
    "    \n",
    "    elif 'skip' in correlation_with_score.lower():  # do not correlate with placement score, output sim score as is\n",
    "        out_val = np.array([np.tanh(np.nanmean(np.arctanh(cx))),px]) # similarity: [r-value, p-value]\n",
    "\n",
    "    return out_val\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func diag peritem - per-q (between) or per-student (within) rval and pval, do not average over questions\\students\n",
    "\n",
    "def func_diagonal_similarity_peritem(df_q_timestamps, placement_by_q, data_in_sl, sim_or_sim_minus_nots='sim',get_reg_coeff=False):\n",
    "    number_of_questions = len(df_q_timestamps['q_number'].unique())  # 16 q in placement exam\n",
    "    corr_result = {}\n",
    "    for student_or_expert in sorted(subject_groups): # order is important- do expert before to allow comparison with student\n",
    "        epi_data = data_in_sl[student_or_expert + 's'][\n",
    "            'test_data']  # in no srm version, epi data is just the non-transformed test data\n",
    "        number_of_subjects = len(epi_data)\n",
    "        corr_result[student_or_expert] = {}\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = None  # timecourse data in roi, do isc on this\n",
    "        corr_result[student_or_expert][\n",
    "            'roi_isc_per_q_vs_group_rval'] = None  # r-value result of isc, questions are in the 'voxels' dim: student vs student, expert vs expert\n",
    "        if 'student' in student_or_expert:\n",
    "            corr_result['student']['roi_isc_per_q_student_vs_expert_rval'] = None  # rval result of pseudo-isc, student vs expert\n",
    "\n",
    "        epi_temporal_mean_per_q = np.zeros(\n",
    "        [number_of_questions, epi_data[0].shape[0], len(epi_data)])  # questions (instead of TR) X voxels X subjects\n",
    "        for this_question in range(number_of_questions):\n",
    "            # print(this_question)\n",
    "            # slice this question: entries for this question, all subjects (expert or student separately)\n",
    "            df_this_q = df_q_timestamps.loc[np.logical_and(df_q_timestamps['student_or_expert'] == student_or_expert,\n",
    "                                                           df_q_timestamps['q_number'] == this_question)]\n",
    "            df_this_q = df_this_q.sort_values(by=['subject'], ascending=True)\n",
    "            # start and end points for this question (TR), for each subject\n",
    "            trs_start = df_this_q['q_start_TR'].values + trs_to_add_to_start\n",
    "            trs_end = df_this_q['q_end_TR'].values + trs_to_add_to_end\n",
    "            assert np.sum((trs_end - trs_start) > 2), 'slice problem in {}-{}-{}-{}'.format(student_or_expert,\n",
    "                                                                                            this_question, trs_end,\n",
    "                                                                                            trs_start)\n",
    "            # extract mean across TRs of epi data per question: vox X subjects\n",
    "            epi_temporal_mean_per_q[this_question, :, :] = np.array(\n",
    "                [np.nanmean(epi_data[i_subject][:, np.int(trs_start[i_subject]):np.int(trs_end[i_subject])], axis=1) for\n",
    "                 i_subject in np.arange(len(trs_start))]).transpose()\n",
    "            # assert not np.sum(np.isnan(epi_temporal_mean_per_q[this_question,:,:])), 'nans here in {}, q{}'.format(student_or_expert,this_question)\n",
    "        # update dict with epi data, mean over TRs\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = epi_temporal_mean_per_q\n",
    "        # run spatial isc with questions as first dim (get per-question value) (output: question X subject)\n",
    "        # student vs mean of student, expert-expert\n",
    "        # run isc\n",
    "        corr_result[student_or_expert]['roi_isc_per_q_vs_group_rval'] = func_isc_in_notebook(epi_temporal_mean_per_q,\n",
    "                                                                                        collapse_subj=False)\n",
    "        # skip bootstrap dist\n",
    "        #corr_result[student_or_expert]['roi_isc_per_q_vs_group_dist']=None\n",
    "\n",
    "        # student vs CLEAN mean of expert (omit patterns of expert that answered wrong)\n",
    "        if 'student' in student_or_expert:\n",
    "            if epi_data is not None:\n",
    "                try:\n",
    "                    expert_all = corr_result['expert']['roi_epi_per_q']  # get data\n",
    "                    if not (('questions' in test_vid_name) or ('placement' in test_vid_name)):\n",
    "                        # 1) non-clean version - for use when no questions are involved - running just on video - qualifies all 'answers'\n",
    "                        expert_clean_collapsed = np.nanmean(expert_all,axis=2)                  \n",
    "                    else:\n",
    "                        # 2) CLEAN version   \n",
    "                        clean_mask = (placement_by_q[\n",
    "                                          'experts'] >= expert_accept_question_threshold).values.transpose()  # mask out wrong answers\n",
    "                        # mask out bad expert responses, replace with nans\n",
    "                        for v in np.arange(expert_all.shape[1]):\n",
    "                            expert_all[:, v, :][np.invert(clean_mask)] = np.nan\n",
    "                        # collapse over experts\n",
    "                        expert_clean_collapsed = np.nanmean(\n",
    "                            np.array([expert_all[:, v, :] for v in np.arange(expert_all.shape[1])]),\n",
    "                            axis=2).transpose()\n",
    "                    # get rval (skip null dist)\n",
    "                    corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_rval'] = func_isc_in_notebook(\n",
    "                        epi_temporal_mean_per_q, collapse_subj=False, external_signal=expert_clean_collapsed)\n",
    "                    \n",
    "                except KeyError: # no expert data\n",
    "                    # zero out student-expert\n",
    "                    corr_result[student_or_expert]['roi_isc_per_q_student_vs_expert_rval']=0\n",
    "                   \n",
    "    if 'experts' in vs_mean_of:\n",
    "        #rval \n",
    "        cx = corr_result['student']['roi_isc_per_q_student_vs_expert_rval'].copy() # questions X students\n",
    "        # pval for the mean value, variance source: subjects\n",
    "        px=np.nan\n",
    "    elif 'students' in vs_mean_of:\n",
    "        cx = corr_result['student']['roi_isc_per_q_vs_group_rval']  # questions X students\n",
    "        # pval for the mean value, variance source: subjects\n",
    "        px=np.nan      \n",
    "    out_val=np.nan\n",
    "    \n",
    "    \n",
    "    if 'skip' in correlation_with_score.lower():\n",
    "        out_val=cx # questions X students - full output, no collapse, no corrw question score\n",
    "    #\n",
    "    #between: output is per-q rval,pval\n",
    "    if 'between' in correlation_with_score.lower():  # correlate similarity score with placement score between subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[0]) # rvals per q\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[0])) # rand dist for each rval \n",
    "        corr_pval_vec = np.zeros(cx.shape[0]) # pval from corr_score_vec and corr_perm_dist, per q\n",
    "        for i_this_q in range(cx.shape[0]):\n",
    "            # rvals and distribution for each, taken from that question\n",
    "            x = cx[i_this_q,:]\n",
    "            y = placement_by_q['students'][i_this_q].values\n",
    "            corr_score_vec[i_this_q],corr_perm_dist[:, i_this_q] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "            if np.isnan(corr_score_vec[i_this_q]) or np.sum(np.isnan(corr_perm_dist[:, i_this_q])):\n",
    "                corr_pval_vec[i_this_q]=np.nan\n",
    "            else:\n",
    "                corr_pval_vec[i_this_q]=isc.p_from_null(corr_score_vec[i_this_q],corr_perm_dist[:, i_this_q],side='right',exact=False,axis=0)\n",
    "        out_val= corr_score_vec,corr_pval_vec # tuple len 2, each item is a (16,) vector  \n",
    "    \n",
    "    #within: output is per-student rval,pval\n",
    "    if 'within' in correlation_with_score.lower():  # correlate similarity score with placement score\n",
    "        # for every subject, use per-q data, correlate similarity to experts (16 vals) with question score (16 vals): return mean over subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[1]) # rvals, vector size number of subjects\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[1])) # rand dist for each rval \n",
    "        corr_pval_vec = np.zeros(cx.shape[1]) # pval from corr_score_vec and corr_perm_dist, per q\n",
    "        corr_trendline_coeff_a=np.zeros(cx.shape[1]) # regression line coefficient a\n",
    "        corr_trendline_coeff_b=np.zeros(cx.shape[1]) # regression line coefficient b\n",
    "        for this_subject in np.arange(number_of_subjects):\n",
    "            # rvals and distribution for each, taken from that subject\n",
    "            # x is sim (or sim nots)\n",
    "            x = cx[:, this_subject]\n",
    "            y = placement_by_q['students'].iloc[this_subject].values  # y is vec q scores this subject\n",
    "            corr_score_vec [this_subject],corr_perm_dist[:, this_subject] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "            if np.isnan(corr_score_vec[this_subject]) or np.sum(np.isnan(corr_perm_dist[:, this_subject])):\n",
    "                corr_pval_vec[this_subject]=np.nan\n",
    "            else:\n",
    "                corr_pval_vec[this_subject]=isc.p_from_null(corr_score_vec[this_subject],corr_perm_dist[:, this_subject],side='right',exact=False,axis=0)\n",
    "            if get_reg_coeff:\n",
    "                regline=np.polyfit(x, y, 1)\n",
    "                corr_trendline_coeff_a[this_subject]=regline[0]\n",
    "                corr_trendline_coeff_b[this_subject]=regline[1]    \n",
    "        if not get_reg_coeff:\n",
    "            out_val = corr_score_vec,corr_pval_vec # tuple len 2, each item is a (n_subj,) vector \n",
    "        else: # get regression coefficients for reg line (a,b, for ax+b)\n",
    "            out_val = corr_score_vec,corr_pval_vec, corr_trendline_coeff_a,corr_trendline_coeff_b # tuple len 4, each item is a (n_subj,) vector, a, b \n",
    "\n",
    "        \n",
    "    return out_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_isfc_similarity(df_q_timestamps, placement_by_q, data_in_sl):\n",
    "    \"\"\"\n",
    "    isfc similarity: knowledge structure \n",
    "    use experts isfc as template for students isfc off-diagonal\n",
    "    \"\"\"\n",
    "    number_of_questions = len(df_q_timestamps['q_number'].unique())  # 16 q in placement exam\n",
    "    corr_result = {}\n",
    "\n",
    "    # first, get ISFC matrices for experts (collapsed) and students (not collapsed)\n",
    "    for student_or_expert in sorted(subject_groups): # order is important - do expert before to allow comparison with student\n",
    "        epi_data = data_in_sl[student_or_expert + 's'][\n",
    "            'test_data']  # in no srm version, epi data is just the non-transformed test data\n",
    "        corr_result[student_or_expert] = {}\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = None  # timecourse data in roi, do isc on this\n",
    "        corr_result[student_or_expert]['q-q-similarity'] = None  # # per participant, question-to-question similarity\n",
    "        epi_temporal_mean_per_q = np.zeros(\n",
    "            [number_of_questions, epi_data[0].shape[0], len(epi_data)])  # questions (instead of TR) X voxels X subjects\n",
    "        \n",
    "        for this_question in range(number_of_questions):\n",
    "            #print(this_question)\n",
    "            #df_q_timestamps\n",
    "            # slice this question: entries for this question, all subjects (expert or student separately)\n",
    "            df_this_q = df_q_timestamps.loc[np.logical_and(df_q_timestamps['student_or_expert'] == student_or_expert,\n",
    "                                                           df_q_timestamps['q_number'] == this_question)]\n",
    "            df_this_q = df_this_q.sort_values(by=['subject'], ascending=True)\n",
    "            \n",
    "            \n",
    "            # start and end points for this question (TR), for each subject\n",
    "            trs_start = df_this_q['q_start_TR'].values + trs_to_add_to_start\n",
    "            trs_end = df_this_q['q_end_TR'].values + trs_to_add_to_end\n",
    "            assert np.sum((trs_end - trs_start) > 2), 'slice problem in {}-{}-{}-{}'.format(student_or_expert,\n",
    "                                                                                            this_question, trs_end,\n",
    "                                                                                            trs_start)\n",
    "            # extract mean across TRs of epi data per question: vox X subjects\n",
    "            epi_temporal_mean_per_q[this_question, :, :] = np.array(\n",
    "                [np.nanmean(epi_data[i_subject][:, np.int(trs_start[i_subject]):np.int(trs_end[i_subject])], axis=1) for\n",
    "                 i_subject in np.arange(len(trs_start))]).transpose()\n",
    "            # assert not np.sum(np.isnan(epi_temporal_mean_per_q[this_question,:,:])), 'nans here in {}, q{}'.format(student_or_expert,this_question)\n",
    "        # update dict with epi data, mean over TRs\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = epi_temporal_mean_per_q\n",
    "        # epi_per_question in original code -> epi_temporal_mean_per_q\n",
    "\n",
    "        if 'expert' in subject_groups:\n",
    "            # calc expert template and use that for ISFC of students,experts\n",
    "            if 'expert' in student_or_expert:\n",
    "                if not (('questions' in test_vid_name) or ('placement' in test_vid_name)):\n",
    "                    # 1) non-clean version - for use when no questions are involved - running just on video - qualifies all 'answers'\n",
    "                    experts_clean_collapsed = np.nanmean(epi_temporal_mean_per_q,axis=2)\n",
    "                else:\n",
    "                    # 2) CLEAN version   \n",
    "                    clean_mask = (placement_by_q[\n",
    "                                      'experts'] >= expert_accept_question_threshold).values.transpose()  # mask out wrong answers\n",
    "                    # mask out bad expert responses - replace with nans\n",
    "                    for v in np.arange(epi_temporal_mean_per_q.shape[1]):\n",
    "                        epi_temporal_mean_per_q[:, v, :][np.invert(clean_mask)] = np.nan\n",
    "                    # mean over experts\n",
    "                    experts_clean_collapsed = np.nanmean(np.array(\n",
    "                        [epi_temporal_mean_per_q[:, v, :] for v in np.arange(epi_temporal_mean_per_q.shape[1])]),\n",
    "                                                      axis=2).transpose()\n",
    "\n",
    "                # update dict (unused, here for clarity)\n",
    "                corr_result[student_or_expert]['template'] = experts_clean_collapsed.copy()\n",
    "\n",
    "            # for both students and experts\n",
    "            # calc ISFC with experts as template\n",
    "            # collapse over experts to create template, don't collapse over students to keep individual\n",
    "            do_isfc_mean_over_subj = True if 'expert' in student_or_expert else False\n",
    "            corr_result[student_or_expert]['isfc_mat'] = \\\n",
    "                func_isfc_in_notebook(corr_result[student_or_expert]['roi_epi_per_q'],\\\n",
    "                collapse_subj=do_isfc_mean_over_subj, external_signal=experts_clean_collapsed)\n",
    "\n",
    "        # for students only: calc standard ISFC, leave one out\n",
    "        if 'student' in student_or_expert:\n",
    "            corr_result[student_or_expert]['isfc_mat'] = \\\n",
    "                func_isfc_in_notebook(corr_result[student_or_expert]['roi_epi_per_q'],\\\n",
    "                collapse_subj=False)\n",
    "\n",
    "\n",
    "    #second, do row-by-row (q-by-q) correlation between st and ex matrices\n",
    "    # get data\n",
    "    students_mat = corr_result['student']['isfc_mat'].copy()\n",
    "    if 'expert' in subject_groups:\n",
    "        experts_mat = corr_result['expert']['isfc_mat'].copy()\n",
    "    number_of_questions = students_mat.shape[0]\n",
    "    number_of_subjects = students_mat.shape[2]\n",
    "    # corr each question in each subject with experts' pattern for question and question-nots\n",
    "    sim_rval = np.zeros([number_of_questions, number_of_subjects])  # similarity for each question, subject\n",
    "    sim_dist = np.zeros([number_of_questions, number_of_subjects, num_perms])\n",
    "    for this_question in np.arange(number_of_questions):\n",
    "        # get (mean of) experts' pattern for this question\n",
    "        if 'expert' in subject_groups:\n",
    "            experts_this_q = experts_mat[this_question, :]\n",
    "            experts_this_q_nots = experts_mat[np.arange(len(experts_mat)) != this_question, :]  # questions-1 X questions\n",
    "        students_this_q_nots = students_mat[np.arange(len(students_mat)) != this_question, :, :]\n",
    "        for this_subject in np.arange(number_of_subjects):\n",
    "            this_student_this_q = students_mat[this_question, :, this_subject]  # sim pattern for this st, this q\n",
    "            group_this_q = np.mean(students_mat[this_question, :, np.arange(students_mat.shape[2]) != this_subject],\n",
    "                                   axis=0)  # sim pattern for all other st, this q (ISC)\n",
    "            # (1) sim_rval: correlate this question sim pattern in expert/group of students, this question sim pattern in student\n",
    "            # but omit same-q corr, will be 1 for both experts and students, drive corr up artificially\n",
    "            x = this_student_this_q[np.arange(len(this_student_this_q)) != this_question].copy()         \n",
    "            if 'experts' in vs_mean_of:\n",
    "                # correlate this-question sim pattern in student, this-question patterns in experts\n",
    "                y_experts = experts_this_q[np.arange(len(experts_this_q)) != this_question].copy()\n",
    "                x_corr_y_rval,x_corr_y_dist=func_corr_and_null_dist(x,y_experts,num_perms=num_perms) \n",
    "            elif 'students' in vs_mean_of:\n",
    "                # correlate this-question sim pattern in student, this-question patterns in group of students\n",
    "                y_students = group_this_q[np.arange(len(this_student_this_q)) != this_question].copy()\n",
    "                x_corr_y_rval,x_corr_y_dist=func_corr_and_null_dist(x,y_students,num_perms=num_perms)\n",
    "            sim_rval[this_question, this_subject] = x_corr_y_rval.copy()\n",
    "            sim_dist[this_question, this_subject,:] = x_corr_y_dist.copy()\n",
    "\n",
    "\n",
    "    # prep for out / corr and out\n",
    "    # rval\n",
    "    cx = sim_rval.copy()  # questions X students\n",
    "    # pval\n",
    "    # collapse across questions and students to get null dist for mean\n",
    "    # sim_dist: questions X subjects X perms\n",
    "    temp=np.tanh(np.nanmean(np.arctanh(sim_dist),axis=0))\n",
    "    null_dist=np.tanh(np.nanmean(np.arctanh(temp),axis=0))\n",
    "    if np.isnan(np.nanmean(cx)) or np.sum(np.isnan(null_dist)):\n",
    "        px=np.nan\n",
    "    else:\n",
    "        px = isc.p_from_null(np.tanh(np.nanmean(np.arctanh(cx))),null_dist,side='right',exact=False)\n",
    "\n",
    "    if 'within' in correlation_with_score.lower():  # correlate similarity score with placement score\n",
    "        # for every subject, use per-q data, correlate similarity to experts (16 vals) with question score (16 vals): return mean over subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[1]) # rvals, vector size number of subjects\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[1])) # rand dist for each rval \n",
    "        for this_subject in np.arange(number_of_subjects):\n",
    "            # rvals and distribution for each, taken from that subject\n",
    "            # x is sim (or sim nots)\n",
    "            x = cx[:, this_subject]\n",
    "            y = placement_by_q['students'].iloc[this_subject].values  # y is vec q scores this subject\n",
    "            corr_score_vec [this_subject],corr_perm_dist[:, this_subject] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "        #rval summary - as in searchlight\n",
    "        rval = isc.compute_summary_statistic(corr_score_vec)\n",
    "        #pval\n",
    "        within_dist = np.tanh(np.nanmean(np.arctanh(corr_perm_dist),axis=1)) # mean over subjects for each rand perm\n",
    "        if np.isnan(rval) or np.sum(np.isnan(within_dist)):\n",
    "            pval=np.nan\n",
    "        else:\n",
    "            pval=isc.p_from_null(rval,within_dist,side='right',exact=False)\n",
    "        out_val = np.array([rval,pval])\n",
    "        \n",
    "        \n",
    "    elif 'skip' in correlation_with_score.lower():  # do not correlate with placement score, output sim score as is\n",
    "        out_val = np.array([np.tanh(np.nanmean(np.arctanh(cx))),px])\n",
    "    \n",
    "    return out_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_isfc_similarity_peritem(df_q_timestamps, placement_by_q, data_in_sl,get_reg_coeff=False):\n",
    "    \"\"\"\n",
    "    isfc similarity:\n",
    "    use experts isfc as template for students isfc off-diagonal\n",
    "    peritem: returns values for all subjects, in 'within' corr, all q in 'between' corr\n",
    "    get_reg_coeff: get trend line coeffients ax+b\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_questions = len(df_q_timestamps['q_number'].unique())  # 16 q in placement exam\n",
    "    corr_result = {}\n",
    "\n",
    "    # first, get ISFC matrices for experts (collapsed) and students (not collapsed)\n",
    "    for student_or_expert in sorted(subject_groups): # order is important - do expert before to allow comparison with student\n",
    "        epi_data = data_in_sl[student_or_expert + 's'][\n",
    "            'test_data']  # in no srm version, epi data is just the non-transformed test data\n",
    "        corr_result[student_or_expert] = {}\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = None  # timecourse data in roi, do isc on this\n",
    "        corr_result[student_or_expert]['q-q-similarity'] = None  # # per participant, question-to-question similarity\n",
    "        epi_temporal_mean_per_q = np.zeros(\n",
    "            [number_of_questions, epi_data[0].shape[0], len(epi_data)])  # questions (instead of TR) X voxels X subjects\n",
    "        for this_question in range(number_of_questions):\n",
    "            # print(this_question)\n",
    "            # slice this question: entries for this question, all subjects (expert or student separately)\n",
    "            df_this_q = df_q_timestamps.loc[np.logical_and(df_q_timestamps['student_or_expert'] == student_or_expert,\n",
    "                                                           df_q_timestamps['q_number'] == this_question)]\n",
    "            df_this_q = df_this_q.sort_values(by=['subject'], ascending=True)\n",
    "            # start and end points for this question (TR), for each subject\n",
    "            trs_start = df_this_q['q_start_TR'].values + trs_to_add_to_start\n",
    "            trs_end = df_this_q['q_end_TR'].values + trs_to_add_to_end\n",
    "            assert np.sum((trs_end - trs_start) > 2), 'slice problem in {}-{}-{}-{}'.format(student_or_expert,\n",
    "                                                                                            this_question, trs_end,\n",
    "                                                                                            trs_start)\n",
    "            # extract mean across TRs of epi data per question: vox X subjects\n",
    "            epi_temporal_mean_per_q[this_question, :, :] = np.array(\n",
    "                [np.nanmean(epi_data[i_subject][:, np.int(trs_start[i_subject]):np.int(trs_end[i_subject])], axis=1) for\n",
    "                 i_subject in np.arange(len(trs_start))]).transpose()\n",
    "            # assert not np.sum(np.isnan(epi_temporal_mean_per_q[this_question,:,:])), 'nans here in {}, q{}'.format(student_or_expert,this_question)\n",
    "        # update dict with epi data, mean over TRs\n",
    "        corr_result[student_or_expert]['roi_epi_per_q'] = epi_temporal_mean_per_q\n",
    "        # epi_per_question in original code -> epi_temporal_mean_per_q\n",
    "\n",
    "        if 'expert' in subject_groups:\n",
    "            # calc expert template and use that for ISFC of students,experts\n",
    "            if 'expert' in student_or_expert:\n",
    "                if not (('questions' in test_vid_name) or ('placement' in test_vid_name)):\n",
    "                    # 1) non-clean version - for use when no questions are involved - running just on video - qualifies all 'answers'\n",
    "                    experts_clean_collapsed = np.nanmean(epi_temporal_mean_per_q,axis=2)\n",
    "                else:\n",
    "                    # 2) CLEAN version   \n",
    "                    clean_mask = (placement_by_q[\n",
    "                                      'experts'] >= expert_accept_question_threshold).values.transpose()  # mask out wrong answers\n",
    "                    # mask out bad expert responses - replace with nans\n",
    "                    for v in np.arange(epi_temporal_mean_per_q.shape[1]):\n",
    "                        epi_temporal_mean_per_q[:, v, :][np.invert(clean_mask)] = np.nan\n",
    "                    # mean over experts\n",
    "                    experts_clean_collapsed = np.nanmean(np.array(\n",
    "                        [epi_temporal_mean_per_q[:, v, :] for v in np.arange(epi_temporal_mean_per_q.shape[1])]),\n",
    "                                                      axis=2).transpose()\n",
    "\n",
    "                # update dict (unused, here for clarity)\n",
    "                corr_result[student_or_expert]['template'] = experts_clean_collapsed.copy()\n",
    "\n",
    "            # for both students and experts\n",
    "            # calc ISFC with experts as template\n",
    "            # collapse over experts to create template, don't collapse over students to keep individual\n",
    "            do_isfc_mean_over_subj = True if 'expert' in student_or_expert else False\n",
    "            corr_result[student_or_expert]['isfc_mat'] = \\\n",
    "                func_isfc_in_notebook(corr_result[student_or_expert]['roi_epi_per_q'],\\\n",
    "                collapse_subj=do_isfc_mean_over_subj, external_signal=experts_clean_collapsed)\n",
    "\n",
    "        # for students only: calc standard ISFC, leave one out\n",
    "        if 'student' in student_or_expert:\n",
    "            corr_result[student_or_expert]['isfc_mat'] = \\\n",
    "                func_isfc_in_notebook(corr_result[student_or_expert]['roi_epi_per_q'],\\\n",
    "                collapse_subj=False)\n",
    "\n",
    "\n",
    "    #second, do row-by-row (q-by-q) correlation between st and ex matrices\n",
    "    # get data\n",
    "    students_mat = corr_result['student']['isfc_mat'].copy()\n",
    "    if 'expert' in subject_groups:\n",
    "        experts_mat = corr_result['expert']['isfc_mat'].copy()\n",
    "    number_of_questions = students_mat.shape[0]\n",
    "    number_of_subjects = students_mat.shape[2]\n",
    "    # corr each question in each subject with experts' pattern for question and question-nots\n",
    "    sim_rval = np.zeros([number_of_questions, number_of_subjects])  # similarity for each question, subject\n",
    "    #sim_dist = np.zeros([number_of_questions, number_of_subjects, num_perms])\n",
    "    for this_question in np.arange(number_of_questions):\n",
    "        # get (mean of) experts' pattern for this question\n",
    "        if 'expert' in subject_groups:\n",
    "            experts_this_q = experts_mat[this_question, :]\n",
    "            experts_this_q_nots = experts_mat[np.arange(len(experts_mat)) != this_question, :]  # questions-1 X questions\n",
    "        students_this_q_nots = students_mat[np.arange(len(students_mat)) != this_question, :, :]\n",
    "        for this_subject in np.arange(number_of_subjects):\n",
    "            this_student_this_q = students_mat[this_question, :, this_subject]  # sim pattern for this st, this q\n",
    "            group_this_q = np.mean(students_mat[this_question, :, np.arange(students_mat.shape[2]) != this_subject],\n",
    "                                   axis=0)  # sim pattern for all other st, this q (ISC)\n",
    "            # (1) sim_rval: correlate this question sim pattern in expert/group of students, this question sim pattern in student\n",
    "            # but omit same-q corr, will be 1 for both experts and students, drive corr up artificially\n",
    "            x = this_student_this_q[np.arange(len(this_student_this_q)) != this_question].copy()         \n",
    "            if 'experts' in vs_mean_of:\n",
    "                # correlate this-question sim pattern in student, this-question patterns in experts\n",
    "                y_experts = experts_this_q[np.arange(len(experts_this_q)) != this_question].copy()\n",
    "                x_corr_y_rval,x_corr_y_dist=func_corr_and_null_dist(x,y_experts,num_perms=0) # skip dist\n",
    "            elif 'students' in vs_mean_of:\n",
    "                # correlate this-question sim pattern in student, this-question patterns in group of students\n",
    "                y_students = group_this_q[np.arange(len(this_student_this_q)) != this_question].copy()\n",
    "                x_corr_y_rval,x_corr_y_dist=func_corr_and_null_dist(x,y_students,num_perms=0)\n",
    "            sim_rval[this_question, this_subject] = x_corr_y_rval.copy()\n",
    "            #sim_dist[this_question, this_subject,:] = x_corr_y_dist.copy()\n",
    "\n",
    "\n",
    "    # prep for out / corr and out\n",
    "    # rval\n",
    "    cx = sim_rval.copy()  # questions X students\n",
    "    # pval\n",
    "    # collapse across questions and students to get null dist for mean\n",
    "    # skip sim_dist\n",
    "    px=np.nan\n",
    "\n",
    "    out_val=np.nan\n",
    "    \n",
    "    \n",
    "    if 'skip' in correlation_with_score.lower():\n",
    "        out_val=cx # questions X students - full output, no collapse, no corrw question score\n",
    "\n",
    "    \n",
    "    #between: output is per-q rval,pval\n",
    "    if 'between' in correlation_with_score.lower():  # correlate similarity score with placement score between subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[0]) # rvals per q\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[0])) # rand dist for each rval \n",
    "        corr_pval_vec = np.zeros(cx.shape[0]) # pval from corr_score_vec and corr_perm_dist, per q\n",
    "        for i_this_q in range(cx.shape[0]):\n",
    "            # rvals and distribution for each, taken from that question\n",
    "            x = cx[i_this_q,:]\n",
    "            y = placement_by_q['students'][i_this_q].values\n",
    "            corr_score_vec[i_this_q],corr_perm_dist[:, i_this_q] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "            if np.isnan(corr_score_vec[i_this_q]) or np.sum(np.isnan(corr_perm_dist[:, i_this_q])):\n",
    "                corr_pval_vec[i_this_q]=np.nan\n",
    "            else:\n",
    "                corr_pval_vec[i_this_q]=isc.p_from_null(corr_score_vec[i_this_q],corr_perm_dist[:, i_this_q],side='right',exact=False,axis=0)\n",
    "        out_val= corr_score_vec,corr_pval_vec # tuple len 2, each item is a (16,) vector  \n",
    "\n",
    "\n",
    "    #within: output is per-student rval,pval\n",
    "    if 'within' in correlation_with_score.lower():  # correlate similarity score with placement score\n",
    "        # for every subject, use per-q data, correlate similarity to experts (16 vals) with question score (16 vals): return mean over subjects\n",
    "        corr_score_vec = np.zeros(cx.shape[1]) # rvals, vector size number of subjects\n",
    "        corr_perm_dist = np.zeros((num_perms,cx.shape[1])) # rand dist for each rval \n",
    "        corr_pval_vec = np.zeros(cx.shape[1]) # pval from corr_score_vec and corr_perm_dist, per q\n",
    "        corr_trendline_coeff_a=np.zeros(cx.shape[1]) # regression line coefficient a\n",
    "        corr_trendline_coeff_b=np.zeros(cx.shape[1]) # regression line coefficient b\n",
    "        for this_subject in np.arange(number_of_subjects):\n",
    "            # rvals and distribution for each, taken from that subject\n",
    "            # x is sim (or sim nots)\n",
    "            x = cx[:, this_subject]\n",
    "            y = placement_by_q['students'].iloc[this_subject].values  # y is vec q scores this subject\n",
    "            corr_score_vec [this_subject],corr_perm_dist[:, this_subject] = func_corr_and_null_dist(x,y,num_perms=num_perms)\n",
    "            #pval\n",
    "            if np.isnan(corr_score_vec[this_subject]) or np.sum(np.isnan(corr_perm_dist[:, this_subject])):\n",
    "                corr_pval_vec[this_subject]=np.nan\n",
    "            else:\n",
    "                corr_pval_vec[this_subject]=isc.p_from_null(corr_score_vec[this_subject],corr_perm_dist[:, this_subject],side='right',exact=False,axis=0)\n",
    "            #coeff\n",
    "            if get_reg_coeff:\n",
    "                regline=np.polyfit(x, y, 1)\n",
    "                corr_trendline_coeff_a[this_subject]=regline[0]\n",
    "                corr_trendline_coeff_b[this_subject]=regline[1]\n",
    "            \n",
    "        # for all rval\n",
    "        if not get_reg_coeff:\n",
    "            out_val = corr_score_vec,corr_pval_vec # tuple len 2, each item is a (n_subj,) vector \n",
    "        else: # get regression coefficients for reg line (a,b, for ax+b)\n",
    "            out_val = corr_score_vec,corr_pval_vec, corr_trendline_coeff_a,corr_trendline_coeff_b # tuple len 4, each item is a (n_subj,) vector, a, b \n",
    "\n",
    "\n",
    "    return out_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fixed params for TEST\n",
      "\n",
      "mpi info\n",
      "<mpi4py.MPI.Intracomm object at 0x7f2a5291e610>\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#def main(argv=None):\n",
    "\n",
    "argv=None\n",
    "# load argv\n",
    "if argv is None:\n",
    "    argv = sys.argv\n",
    "\n",
    "# declare all global params (use global for searchlight via MPI)\n",
    "global test_vid_week\n",
    "global test_vid_name\n",
    "global similarity_type\n",
    "global correlation_with_score\n",
    "global vs_mean_of\n",
    "global sl_rad\n",
    "global num_perms\n",
    "global threshold\n",
    "global train_vid_week\n",
    "global train_vid_name\n",
    "global do_srm\n",
    "\n",
    "# load params\n",
    "\n",
    "if use_test_params or use_roi:\n",
    "    print('Loading fixed params for TEST')\n",
    "    train_vid_week = None\n",
    "    train_vid_name = None\n",
    "    do_srm = False\n",
    "    test_vid_week = 'wk6'\n",
    "    test_vid_name = 'placement'\n",
    "    similarity_type = 'diagonal'\n",
    "    correlation_with_score = 'within'\n",
    "    vs_mean_of = 'student-vs-experts'\n",
    "    sl_rad = int(2)\n",
    "    num_perms = int(30)\n",
    "    threshold = float(0.05)\n",
    "    \n",
    "else: # use command line params\n",
    "    if 'wk' in sys.argv[1]:\n",
    "        train_vid_week = sys.argv[1]\n",
    "        train_vid_name = sys.argv[2]\n",
    "        do_srm = True\n",
    "        print('Loading params from command line')\n",
    "        print('SRM: on')\n",
    "        print('SRM training video: {}-{}'.format(train_vid_week,train_vid_name))\n",
    "    else:\n",
    "        train_vid_week = None # train data for SRM (None = no SRM)\n",
    "        train_vid_name = None\n",
    "        do_srm = False\n",
    "    # test data for SRM (if no SRM, use test data only)\n",
    "    test_vid_week = sys.argv[3]  # 'wk6'\n",
    "    test_vid_name = sys.argv[4]  # 'placement'\n",
    "    # similarity: 1st order / 2nd order\n",
    "    similarity_type = sys.argv[5]  #diagonal\n",
    "    # correlation with score: skip/within-subj\n",
    "    correlation_with_score = sys.argv[6]\n",
    "    # vs mean of: calc similarity for student-vs-experts / student-vs-students\n",
    "    vs_mean_of = sys.argv[7]\n",
    "    # sl_rad # searchligh radius\n",
    "    sl_rad = int(sys.argv[8])\n",
    "    # perms for stats: 0-1000\n",
    "    num_perms = int(sys.argv[9])\n",
    "    # maps p-value threshold (FDR corrected)\n",
    "    threshold = float(0.05)\n",
    "    # print\n",
    "    print('Input video (=SRM test): {}-{}'.format(test_vid_week,test_vid_name))\n",
    "    print('Similarity type: {}'.format(similarity_type))\n",
    "    print('Correlate with score: {}'.format(correlation_with_score))\n",
    "    print('student-experts or student-students: {}'.format(vs_mean_of))\n",
    "    print('Searchlight edge: {}, size = {} voxels'.format(sl_rad,(1+2*sl_rad)**3))\n",
    "    print('Number of perms: {}'.format(num_perms))\n",
    "    print('p-value threshold for maps, FDR corrected: {}'.format(threshold))\n",
    "\n",
    "\n",
    "# MPI - parallelization\n",
    "global comm,rank,size\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank  # rank = comm.Get_rank()\n",
    "size = comm.size  # size = comm.Get_size()\n",
    "print()\n",
    "print('mpi info')\n",
    "print(comm)\n",
    "print(rank)\n",
    "print(size)\n",
    "\n",
    "\n",
    "# load brain mask\n",
    "global brain_mask_3mm\n",
    "brain_mask_3mm = io.load_boolean_mask(const_mni_brain_file_name, lambda x: x > 0.05)  # check edges!\n",
    "\n",
    "# build dict for all filenames, nested\n",
    "global student_and_expert_files\n",
    "student_and_expert_files = listdir(input_fslfeat_students_path) + listdir(input_fslfeat_experts_path)\n",
    "global task_name_template\n",
    "#task_name_template = \"(s\\d{3})_(wk\\d+)_([0-9a-zA-Z]*)_mni\"  # template for data preprocessed with FSL\n",
    "task_name_template = \"(s\\d{3})_(wk\\d+)_([0-9a-zA-Z]*)_6motion_mni\"  # template for data preprocessed with FSL + regressout 6 motion\n",
    "global filenames_dict\n",
    "filenames_dict = build_dict_filenames(student_and_expert_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read logs and get behavioral data (placement and isq)\n",
    "global df_q_timestamps, placement_by_q,good_students,good_experts\n",
    "\n",
    "df_q_timestamps, placement_by_q = read_placement_logs()\n",
    "# if 'rtregout' in correlation_with_score.lower(): # use grades after regressing out RT\n",
    "#     placement_by_q['students']=placement_by_q['students_clean']\n",
    "good_students = placement_by_q['students'].index.tolist()\n",
    "good_experts = placement_by_q['experts'].index.tolist()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for questions\n",
    "global expert_accept_question_threshold,trs_to_add_to_end,trs_to_add_to_start\n",
    "expert_accept_question_threshold = 2  # for expert pattern used to compare student to, only consider equal-or-above-threshold (correct) answers: use 0 to qualify all answers\n",
    "# trim first 8s of question (before response)\n",
    "trs_to_add_to_end = 0  # pattern of question includes TRs after answer (0s)\n",
    "trs_to_add_to_start = 4  # pattern of question starts TRs after question start time (8s)\n",
    "# set parameters for searchlight\n",
    "# The size of the searchlight's radius, excluding the center voxel. This means the total volume size of the searchlight, if using a cube, is defined as: ((2 * sl_rad) + 1) ^ 3.\n",
    "#sl_rad = 3  # searchlight size (of each edge) will be 1+2*sl_rad; sl_rad=1->27 voxels, =2->125, =3->343 voxels #set by argv\n",
    "global max_blk_edge,pool_size,sl_mask,nfeature,niter\n",
    "max_blk_edge = 10  # size of block searchlight distributes\n",
    "pool_size = 1  # cores per task\n",
    "sl_mask = brain_mask_3mm\n",
    "\n",
    "if do_srm:\n",
    "    # sanity check for SRM parameters\n",
    "    nfeature = 20  # number of features in SRM for each searchlight\n",
    "    niter = 20  # number of iterations in SRM\n",
    "    if nfeature > (1 + 2 * sl_rad) ** 3:\n",
    "        print ('nfeature truncated')\n",
    "        nfeature = int((1 + 2 * sl_rad) ** 3)\n",
    "else:  # no srm\n",
    "    niter = 0\n",
    "    nfeature = 0\n",
    "\n",
    "# sanity check for searchlight parameters\n",
    "if sl_rad <= 0:\n",
    "    raise ValueError('sl_rad must be positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experts\n",
      "Rank 0: Loading subjects -> [0 1 2 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/0.9.1/lib/python3.7/site-packages/scipy/stats/stats.py:2279: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  np.expand_dims(sstd, axis=axis))\n",
      "/jukebox/pkgs/PYGER/0.9.1/lib/python3.7/site-packages/scipy/stats/stats.py:2279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.expand_dims(sstd, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students\n",
      "Rank 0: Loading subjects -> [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "\n",
      "Test set, 19 subjects, load time: 116.78s\n",
      "Test on files:\n",
      "['/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s102_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s103_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s105_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s106_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s107_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s108_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s110_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s111_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s113_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s114_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s116_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s118_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s120_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s121_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s122_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s125_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s126_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s127_wk6_placement_6motion_mni.nii.gz', '/mnt/bucket/labs/hasson/meshulam/onlineL/pred20/scan_data_nii/students_mni/6motion/s129_wk6_placement_6motion_mni.nii.gz']\n",
      "Searchlight cube edge size is 5 -> 125 voxels\n",
      "Number of subjects: 23\n",
      "Number of student datasets: 19, expert datasets: 4.\n",
      "Number of TRs in first list item: 446\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "# epi_list: data for searchlight\n",
    "# first half: train, second half: test (if SRM)\n",
    "# first loc_split items in train/test: students, the rest-experts\n",
    "global subject_groups, epi_list,loc_split\n",
    "if ('questions' in test_vid_name): # no expert data for questions\n",
    "    subject_groups = ['student']    \n",
    "    all_epi_data = load_epi_data(['students'])\n",
    "else:\n",
    "    subject_groups = ['expert','student'] \n",
    "    all_epi_data = load_epi_data(['students','experts'])\n",
    "epi_list = []\n",
    "loc_split = len(all_epi_data['students']['test_data'])\n",
    "if do_srm:\n",
    "    epi_list += all_epi_data['students']['train_data']\n",
    "    epi_list += all_epi_data['experts']['train_data']\n",
    "epi_list += all_epi_data['students']['test_data']\n",
    "epi_list += all_epi_data['experts']['test_data']\n",
    "\n",
    "# print searchlight info\n",
    "if rank == 0:\n",
    "    print_sl_info(epi_list,sl_rad,nfeature,loc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of perms is set to 1000\n"
     ]
    }
   ],
   "source": [
    "# set params\n",
    "use_roi=True\n",
    "min_number_of_vox_in_roi=5\n",
    "do_srm = False # for standard funcs only, untested\n",
    "test_vid_week = 'wk6'\n",
    "test_vid_name = 'placement'\n",
    "sl_rad = int(2)\n",
    "threshold = float(0.05)\n",
    "# set epi list /l\n",
    "l=epi_list\n",
    "epi_list = l\n",
    "\n",
    "bcast_var=[niter, nfeature, loc_split]\n",
    "d1, d2, d3, ntr_train = l[0].shape  # var ntr_train unused\n",
    "nvx = d1 * d2 * d3  # number of vox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity_type:  \n",
    "+ 'diagonal' per-question \n",
    "+ 'isfc' knowledge structure\n",
    "\n",
    "do_collapse:\n",
    "+ 'True' collapse across items (for analysis) & write to file df_rois_similarity.csv\n",
    "+ 'False' do not collapse (for plots) & write to files df_rois_similarity_per_question_and_student.csv' (skip) , \n",
    "\n",
    "correlation_with_score:\n",
    "+ 'skip': raw similarity-to-class and similarity-to-experts\n",
    "+ 'within': correlation between similarity and exam score\n",
    "+ 'direct': correlation between similarity-to-class and similarity-to-experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of perms is set to 1\n",
      "Corr: skip\n",
      "Type: isfc\n",
      "thr: all\n",
      "vs mean of: student-vs-experts\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-experts, 2.628738 s\n",
      "vs mean of: student-vs-students\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-students, 3.163656 s\n",
      "Corr isfc complete\n",
      "Type: diagonal\n",
      "thr: all\n",
      "vs mean of: student-vs-experts\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-experts, 1.991650 s\n",
      "vs mean of: student-vs-students\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-students, 2.074834 s\n",
      "Corr diagonal complete\n",
      "skip-subjects complete\n",
      "number of perms is set to 1000\n",
      "Corr: within\n",
      "Type: isfc\n",
      "thr: all\n",
      "vs mean of: student-vs-experts\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-experts, 267.625638 s\n",
      "vs mean of: student-vs-students\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-students, 265.286766 s\n",
      "Corr isfc complete\n",
      "Type: diagonal\n",
      "thr: all\n",
      "vs mean of: student-vs-experts\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-experts, 37.342575 s\n",
      "vs mean of: student-vs-students\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-students, 37.117988 s\n",
      "Corr diagonal complete\n",
      "within-subjects complete\n",
      "number of perms is set to 1000\n",
      "Corr: direct\n",
      "Type: isfc\n",
      "Type: diagonal\n",
      "thr: all\n",
      "vs mean of: student-vs-experts\n",
      "ROI Cingulate-ant-bilateral: 1092 voxels\n",
      "students-test_data\n",
      "out shape: (1092, 446)\n",
      "experts-test_data\n",
      "out shape: (1092, 642)\n",
      "ROI STG-post-bilateral: 856 voxels\n",
      "students-test_data\n",
      "out shape: (856, 446)\n",
      "experts-test_data\n",
      "out shape: (856, 642)\n",
      "ROI Angular-bilateral: 1783 voxels\n",
      "students-test_data\n",
      "out shape: (1783, 446)\n",
      "experts-test_data\n",
      "out shape: (1783, 642)\n",
      "ROI Heschls-bilateral: 454 voxels\n",
      "students-test_data\n",
      "out shape: (454, 446)\n",
      "experts-test_data\n",
      "out shape: (454, 642)\n",
      "ROI Hippocampus-bilateral: 668 voxels\n",
      "students-test_data\n",
      "out shape: (668, 446)\n",
      "experts-test_data\n",
      "out shape: (668, 642)\n",
      "ROI Amygdala-bilateral: 318 voxels\n",
      "students-test_data\n",
      "out shape: (318, 446)\n",
      "experts-test_data\n",
      "out shape: (318, 642)\n",
      "ROI Precuneous-bilateral: 2845 voxels\n",
      "students-test_data\n",
      "out shape: (2845, 446)\n",
      "experts-test_data\n",
      "out shape: (2845, 642)\n",
      "ROI Intracalcarine-bilateral: 925 voxels\n",
      "students-test_data\n",
      "out shape: (925, 446)\n",
      "experts-test_data\n",
      "out shape: (925, 642)\n",
      "All ROIs in student-vs-experts, 36.784925 s\n",
      "vs mean of: student-vs-students\n",
      "Corr diagonal complete\n",
      "direct-subjects complete\n",
      "Done in 654.018940 s\n",
      "/mnt/bucket/labs/hasson/meshulam/onlineL/shared/pickles/df_rois_similarity.csv saved\n"
     ]
    }
   ],
   "source": [
    "# for Figure 4,5 ('placement'), run 'within'\n",
    "# for Figure 3b ('direct'), run 'direct' & do_collapse=True\n",
    "\n",
    "do_collapse=True # True: collapse over subjects and questions, False: between yields per-q, within yields per-student\n",
    "\n",
    "\n",
    "sim_df=pd.DataFrame() # results\n",
    "t0=time.time()\n",
    "for correlation_with_score in ['skip','within','direct']:\n",
    "\n",
    "    if 'skip' in correlation_with_score:\n",
    "        num_perms = int(1)\n",
    "    else:\n",
    "        num_perms = int(1000) \n",
    "    print('number of perms is set to {}'.format(num_perms))\n",
    "\n",
    "    \n",
    "    print('Corr: '+correlation_with_score)\n",
    "    if not do_collapse: # not grand mean over q and st\n",
    "        #sim_df=pd.DataFrame() # init separately for within\n",
    "        if 'within' in correlation_with_score:\n",
    "            print('Get per-student results')\n",
    "        elif 'skip' in correlation_with_score:\n",
    "            print('Get per-student per-question results, no correlation with score')\n",
    "        elif 'direct' in correlation_with_score:\n",
    "            print('correlate each student with st,ex then correlate 20 st-st and 20 st-ex vals in each q')\n",
    "\n",
    "            \n",
    "    for similarity_type in ['isfc','diagonal']:\n",
    "    #for similarity_type in ['diagonal']:\n",
    "        print('Type: '+similarity_type)\n",
    "        # use diagonal func to calc 'direct'\n",
    "        if 'direct' in correlation_with_score and 'isfc' in similarity_type:\n",
    "            continue\n",
    "        for all_or_thr in ['all']: # all voxels in roi\n",
    "            print('thr: '+all_or_thr)\n",
    "            for vs_mean_of in ['student-vs-experts','student-vs-students']:\n",
    "                print('vs mean of: '+vs_mean_of)\n",
    "                t1=time.time()\n",
    "                # skip st-vs-st if direct\n",
    "                if 'direct' in correlation_with_score and 'student-vs-students' in vs_mean_of:\n",
    "                    break\n",
    "                for this_roi_name in masks.keys():\n",
    "                    #for this_roi_hemi in masks[this_roi_name][all_or_thr+'_voxels'].keys():\n",
    "                    for this_roi_hemi in ['bilateral']:\n",
    "                        # get mask\n",
    "                        this_roi_mask=masks[this_roi_name][all_or_thr+'_voxels'][this_roi_hemi].copy()\n",
    "                        # make sure there's enough voxels in this roi mask\n",
    "                        if np.int(np.sum(this_roi_mask))<min_number_of_vox_in_roi:\n",
    "                            break\n",
    "                        # reset data and reshape\n",
    "                        data_in_sl = dict()\n",
    "                        data_in_sl['students'] = {}\n",
    "                        data_in_sl['students']['test_data'] = []\n",
    "                        data_in_sl['experts'] = {}\n",
    "                        data_in_sl['experts']['test_data'] = []\n",
    "                        tt_data = 'test_data'\n",
    "\n",
    "                        # first half of dataset list ('test') is students, second half experts\n",
    "                        # also do reshape\n",
    "                        for s in epi_list[:bcast_var[2]]:  # st\n",
    "                            if s is not None:\n",
    "                                tt_ntr = s.shape[3]  # number of TRs, can vary between subjects\n",
    "                                data_in_sl['students'][tt_data].append(np.reshape(s, (nvx, tt_ntr)))\n",
    "                        for s in epi_list[bcast_var[2]:]:  # ex\n",
    "                            if s is not None:\n",
    "                                tt_ntr = s.shape[3]\n",
    "                                data_in_sl['experts'][tt_data].append(np.reshape(s, (nvx, tt_ntr)))\n",
    "\n",
    "\n",
    "                        if use_roi: # for ROI instead of whole-brain\n",
    "                            data_in_sl = func_roi_data(data_in_sl,this_roi_mask,this_roi_name,this_roi_hemi)\n",
    "                        \n",
    "                        # run corr\n",
    "                        if 'diagonal' in similarity_type: # same-question analysis\n",
    "                            if not do_collapse: \n",
    "                                ret_val = func_diagonal_similarity_peritem(df_q_timestamps, placement_by_q, data_in_sl,sim_or_sim_minus_nots='sim',get_reg_coeff=True)\n",
    "                            else:\n",
    "                                ret_val = func_diagonal_similarity(df_q_timestamps, placement_by_q, data_in_sl,sim_or_sim_minus_nots='sim')\n",
    "                        elif 'isfc' in similarity_type: # knowledge structure analysis\n",
    "                            if not do_collapse:\n",
    "                                ret_val = func_isfc_similarity_peritem(df_q_timestamps, placement_by_q, data_in_sl,get_reg_coeff=True)\n",
    "                            else:\n",
    "                                ret_val = func_isfc_similarity(df_q_timestamps, placement_by_q, data_in_sl)\n",
    "                            \n",
    "\n",
    "                        if not do_collapse:\n",
    "                            #if 'within' in correlation_with_score pd contains data per subject, ind is subj sorted\n",
    "                            #if 'between' in correlation_with_score pd contains data per question, ind is q\n",
    "                            # if 'skip' in correlation_with_score pd contains data per q and s, no corrw\n",
    "                            # students\\question num, not id, sorted\n",
    "                            if 'skip' in correlation_with_score:#skip, ret_val is questions x subjects\n",
    "                                temp_sim=pd.DataFrame()\n",
    "                                for this_subject in range(ret_val.shape[1]):\n",
    "                                    temp_sim_s=pd.DataFrame({'question': range(ret_val.shape[0]),\\\n",
    "                                           'subject': this_subject,\\\n",
    "                                           'corr_w_score': correlation_with_score,\\\n",
    "                                           'score_type': 'placement',\\\n",
    "                                           'sim_type': similarity_type,\\\n",
    "                                           'roi_type': 'anatomical',\\\n",
    "                                           'vs_mean_of': vs_mean_of,\\\n",
    "                                           'roi_name': this_roi_name,\\\n",
    "                                           'roi_hemi': this_roi_hemi,\\\n",
    "                                           'n_voxels':data_in_sl['students']['test_data'][0].shape[0],\\\n",
    "                                           'rval':ret_val[:,this_subject],\\\n",
    "                                           'pval':np.NaN})\n",
    "                                    temp_sim=temp_sim.append(temp_sim_s)\n",
    "                            else: # between or within, not skip\n",
    "                                # delete coeff if get_reg_coeff is False\n",
    "                                temp_sim=pd.DataFrame({'ind': range(len(ret_val[0])),\\\n",
    "                                                   'corr_w_score': correlation_with_score,\\\n",
    "                                                   'score_type': 'placement',\\\n",
    "                                                   'sim_type': similarity_type,\\\n",
    "                                                   'roi_type': 'anatomical',\\\n",
    "                                                   'vs_mean_of': vs_mean_of,\\\n",
    "                                                   'roi_name': this_roi_name,\\\n",
    "                                                   'roi_hemi': this_roi_hemi,\\\n",
    "                                                   'n_voxels':data_in_sl['students']['test_data'][0].shape[0],\\\n",
    "                                                   'coeff_a':ret_val[2],\\\n",
    "                                                   'coeff_b':ret_val[3],\\\n",
    "                                                   'rval':ret_val[0],\\\n",
    "                                                   'pval':ret_val[1]})\n",
    "                        else: # do_collapse, standard: get one rval,pval for each roi\n",
    "                            temp_sim=pd.DataFrame({'corr_w_score': correlation_with_score,\\\n",
    "                                               'score_type': 'placement',\\\n",
    "                                               'sim_type': similarity_type,\\\n",
    "                                               'roi_type': 'anatomical',\\\n",
    "                                               'vs_mean_of': vs_mean_of,\\\n",
    "                                               'roi_name': this_roi_name,\\\n",
    "                                               'roi_hemi': this_roi_hemi,\\\n",
    "                                               'n_voxels':data_in_sl['students']['test_data'][0].shape[0],\\\n",
    "                                               'rval':ret_val[0],\\\n",
    "                                               'pval':ret_val[1]},index=[0])\n",
    "                        sim_df=sim_df.append(temp_sim)\n",
    "                t2=time.time()\n",
    "                print('All ROIs in {}, {:02f} s'.format(vs_mean_of,t2-t1))   \n",
    "        print('Corr {} complete'.format(similarity_type))\n",
    "    print('{}-subjects complete'.format(correlation_with_score))\n",
    "print('Done in {:02f} s'.format(t2-t0))\n",
    "\n",
    "#SAVE\n",
    "if not do_collapse:\n",
    "    if not 'skip' in correlation_with_score:\n",
    "        df_fname=join(pickles_path,'df_rois_similarity_per_item.csv')\n",
    "    else:\n",
    "        df_fname=join(pickles_path,'df_rois_similarity_per_question_and_student.csv') # for example scatterplot\n",
    "else: \n",
    "    df_fname=join(pickles_path,'df_rois_similarity.csv') #\n",
    "\n",
    "    \n",
    "sim_df.to_csv(df_fname,index=False)\n",
    "print ('{} saved'.format(df_fname))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
